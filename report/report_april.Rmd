---
title: "Report-April"
author: "Lucas Wei√ü"
date: "4/2/2022"
output: 
  html_document:
    css: "style.css"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# load packages -------------------------------------------------------------------------------

library(data.table)
library(dtplyr)
library(tidyverse)
library(caret)
```


```{r single model}
load(file="../output/ranger_single.RData")
load(file="../output/freq_plot_2020.Rdata")
```

### Model Setup

The current model includes data from all parts of Germany and all data points.
The data cleaning process however removed a big part of the data. Namely all the plots where the crop type was not available, grassland, mixed or otherwise without price data attached. 
The training data set includes all the years from 2005 up until 2019. The test data set is all the data in the year 2020.

Here we see the accuracy and the kappa value for the current model.

<div class="acc">
<table>
<tr>
<td>Accuracy</th>
<td>Kappa</th>
<td>AccuracyLower</th>
<td>AccuracyUpper</th>
<td>AccuracyNull</th>
</tr>
<tr>
<td>0.5096595</th>
<td>0.2921844</th>    
<td>0.5067361</th>
<td>0.5125824</th>
<td>0.3694392</th>
</tr>
</table>
</div>

<br>
We can see the accuracy lies at around 0.5 and the kappa value at around 0.3 

### Predictions per Crop Type

When we take a look at the predictions per class we see that the model really prefers to predict the crop types 2 3 and 4.
These include:

<div class="ctype">
<table>
<tr>
<td>2</th>
<td>3</th>
<td>4</th>
</tr>
<tr>
<td>Maize</th>
<td>Wheat</th>
<td>Barley</th>
</tr>
</table>
</div>

<br>

Comparing this result with the distribution of crop types within the data we can see a pattern. The representation of crop type frequency and prediction goes in the same direction.

```{r predictions per class}
plot(res$predictions, las = 2, main="Number of predictions per Class")
(ctype_by_year_plot)+
  labs(x="Crop Type", y="Count")
```

### Variable Importance 

Taking a look at variable importance, we see that price is a huge predictor. 
My first thought was, that there should be a strong correlation between price and the 3 most frequent crop types, which would explain this trend.

```{r variable importance}
var_importance <- res$variable.importance
barplot(var_importance[order(var_importance, decreasing = TRUE)], las = 2, main="Variable importance")
```

Taking a look at the average price per crop type (the red line) and the frequency of each crop type over the years, we can see a pattern that I did not expect. 
The price for the most important crop types is very low in contrast to some crops that are not occurring that much.


```{r correlation}
load(file="../output/correlation.RData")
(corr)
```

### Accuracy for each Crop Type

Nevertheless, price is a very strong predictor for the model, followed by the previous crop type on the same plot of land.
In the next figure, we can see the different accuracy for each crop type.
Here we can see that those crops with high prices have a very high accuracy, while the most frequent crops that also don't have high prices attached to them, don't have a high accuracy.


```{r accuracy for each class}
ggplot(data=class_accuracy, aes(x=pred.predictions, y=acc)) +
  geom_bar(stat="identity")+
  ggtitle("Accuracy for each class")+
  labs(x="Crop Type", y="Accuracy")
```

### Time Series Cross Validation

And finally, taking a look at time series cross validation.
I chose to train on the data set that includes the years 2005-2019 and test in the year 2020.
But I also wanted to include Time Series Cross Validation.
But for that I had to write a cross validation algorithm myself, which would take a huge amount of time.
That's why I wanted to show, that including a long enough test data set that includes enough years of training time, the model will not improve any further. Which then indicates that my approach with taking 15 years of training data and test on 1 year would be good enough.

In the following picture we can see what I wanted to see in the red line. The accuracy would increase with each year that got taken into account but then saturated with enough training data.
In reality we see the blue and black line which represent Kappa and accuracy respectively. 
The odd part is that there is no saturation after an increase but a rather random pattern of increase and decrease of accuracy.

```{r}
load(file="../TSCVres.RData")

df <- data.frame(num_years=unlist(results[[1]]),acc=unlist(results[[2]]), kappa=unlist(results[[3]]) )
df$test <- c(0.1, 0.4, 0.5, 0.55, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.65, 0.65, 0.65, 0.65)
ggplot(data=df ) +
  geom_line(aes(x=num_years, y=acc, group=1))+
  geom_point(aes(x=num_years, y=acc, group=1))+
  geom_line(aes(x=num_years, y=kappa, group=1), color="blue")+
  geom_point(aes(x=num_years, y=kappa, group=1), color="blue")+
  geom_line(aes(x=num_years, y=test, group=1), color="red")+
  geom_point(aes(x=num_years, y=test, group=1))+
  ylim(0,1)+
  ggtitle("Accuracy for number of years includet in the training data set")+
  labs(x="Number of Years", y="Accuracy")
```


This made me think.
Taking a closer look we can see that the highest accuracy was for the test years of 2012 and 2016.
Comparing this result with the average price over the years shows that the years 2012 and 2016 are near the peak and near the valley of price development over the past years.
I am not sure if this would explain what we saw with the accuracy. But potentially the argument could be made, that with the price peak and highest prices around 2012 the model predicted the best by using price and also when prices are the lowest the model is good at estimating the crop types.
But then again, the difference in accuracy is not that high.

```{r mean price}
load(file="../output/mean_price_crops.RData")
(mean_price)+
  labs(x="Year", y="Price")
```